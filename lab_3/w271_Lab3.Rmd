---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Group Lab 3'
subtitle: 'Due Sunday 8 August 2021 11:59pm'
geometry: margin=1in
output:
  pdf_document:
    latex_engine: xelatex
  number_sections: yes
  html_document: default
  toc: yes
fontsize: 11pt
---

## Instructions (Please Read Carefully):

* Submit by the due date. **Late submissions will not be accepted**

* 20 page limit (strict)

* Do not modify fontsize, margin or line-spacing settings

* One student from each group should submit the lab to their student github repo by the deadline

* Submit two files:
    
    1. A pdf file that details your answers. Include all R code used to produce the answers
    
    2. The R markdown (Rmd) file used to produce the pdf file
  
    The assignment will not be graded unless **both** files are submitted
      
* Name your files to include all group members' names. For example, if the students' names are Stan Cartman and Kenny Kyle, name your files as follows:

    * `StanCartman_KennyKyle_Lab3.Rmd`
    * `StanCartman_KennyKyle_Lab3.pdf`
            
* Although it sounds obvious, please write your name on page 1 of your pdf and Rmd files

* All answers should include a detailed narrative; make sure that your audience can easily follow the logic of your analysis. All steps used in modelling must be clearly shown and explained; do not simply 'output dump' the results of code without explanation 

* If you use libraries and functions for statistical modeling that we have not covered in this course, you must provide an explanation of why such libraries and functions are used and reference the library documentation

* For mathematical formulae, type them in your R markdown file. Do not e.g. write them on a piece of paper, snap a photo, and use the image file

* Incorrectly following submission instructions results in deduction of grades

* Students are expected to act with regard to UC Berkeley Academic Integrity.


\newpage

# U.S. traffic fatalities: 1980-2004

In this lab, you are asked to answer the question **"Do changes in traffic laws affect traffic fatalities?"**  To do so, you will conduct the tasks specified below using the data set *driving.Rdata*, which includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economic and demographic variables are also included. The description of the each of the variables in the dataset is come with the dataset.

1. (30%) Load the data. Provide a description of the basic structure of the dataset, as we have done throughout the semester. Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable *totfatrte* and the potential explanatory variables. You need to write a detailed narrative of your observations of your EDA. *Reminder: giving an "output dump" (i.e. providing a bunch of graphs and tables without description and hoping your audience will interpret them) will receive a zero in this exercise.*

**Response:**
The EDA below displays many different graphs and tables of the data. The first table if the correlation plot of all the variables. The dependent variable of interest is the totfatrte, which is the total fatality rate of driving incidents. From the correlation plot, the variables that seem to best correlate with this variable are, the year, seatbelt, minage, gdl, zerotol, vehiclemiles. All these variables are negatively correlated with the dependent variable. There are some variables that are strongly correlated with the dependent variable in a positive way and these include unem, perc14_24, and vehicmilespc. Since some of the speedlimit correlated go from being positive and negative, it seems that the data is inconclusive here on how changing the speed limit may affect driving incidents. Also, it is important to note that all the other measurements of fatality are highly correlated with the dependent variable, however these variables are not independent of one another and are essentially part of the same thing, therefore we will not use them in the model. 

It is also important to note that there is no missing data from the data, and the data goes from 1980 through 2004 and includes 48 different states. The data is set up to have all the data in the time frame of 1980 to 2004 for each state, therefore we can see the changes over time for each individual state and model all the changes together in one model. Only the variables from the data that were discussed above for having a strong correlation to the dependent variable will be analyzed from here on out. 

Since the year was negatively correlated I was interested to model the change in totfatrte over time. All the states were modeled on plots, however for sake of conducting a concise analysis only one plot of 12 different states is shown below. This plot shows the different states over time. We are not necessarily interested on how an individual model's data looks, but rather the overall change, and overall the data looks like it does slightly decrease over the 25 year span. The other data plots that are not shown showed similar plots. 

Lastly, the histograms of all the variable in consideration for the model are shown below. It is difficult to assess what some of the histograms mean, since we do not have information on what some of these variable acronyms mean. However, the point of plotting the histograms of these variables is to see any extreme distribution in the data. Most of the data is generally normal, mainly a lot of the data has right sided tails which may lead to a transformation of that variable in order to obtain a more normal distribution, however these transformations will be discussed in the model formulation part of the lab. 


```{r}
library(plm)
library(funModeling) 
library(tidyverse) 
library(Hmisc)
library(ggplot2)
library(forecast)
library(tseries)
library(corrr)


```


```{r}

load("driving.RData", ex <- new.env())
ls.str(ex) 

head(data)

df <- pdata.frame(data, index=c("state", 'year'))
head(df, 20)
```


```{r}
correlate(data)

sum(is.na(data))

summary(data)

```

```{r}
data1 <- subset(data, state == 1) 
data2 <- subset(data, state == 2)
data3 <- subset(data, state == 3)
data4 <- subset(data, state == 4) 
data5 <- subset(data, state == 5)
data6 <- subset(data, state == 6) 
data7 <- subset(data, state == 7) 
data8 <- subset(data, state == 8)
data9 <- subset(data, state == 9)
data10 <- subset(data, state == 10) 
data11 <- subset(data, state == 11)
data12 <- subset(data, state == 12)

#plot the first data series using plot()
plot(data1$year, data1$totfatrte, type="o", col="blue", pch="o", ylab="y", lty=1, ylim=c(0,50), xlim=c(1979,2005))

#add third data series to the same chart using points() and lines()
points(data3$year, data3$totfatrte, col="dark red",pch="+")
lines(data3$year, data3$totfatrte, col="dark red", lty=3)

points(data4$year, data4$totfatrte, col="red", pch="*")
lines(data4$year, data4$totfatrte, col="red",lty=2)

points(data5$year, data5$totfatrte, col="purple", pch="-")
lines(data5$year, data5$totfatrte, col="purple",lty=1)

points(data6$year, data6$totfatrte, col="orange", pch="-")
lines(data6$year, data6$totfatrte, col="orange",lty=4)

points(data7$year, data7$totfatrte, col="pink", pch="-")
lines(data7$year, data7$totfatrte, col="pink",lty=2)

points(data8$year, data8$totfatrte, col="green", pch="-")
lines(data8$year, data8$totfatrte, col="green",lty=4)

points(data9$year, data9$totfatrte, col="dark blue", pch="-")
lines(data9$year, data9$totfatrte, col="dark blue",lty=4)

points(data10$year, data10$totfatrte, col="dark green", pch="~")
lines(data10$year, data10$totfatrte, col="dark green",lty=4)

points(data11$year, data11$totfatrte, col="hot pink", pch="#")
lines(data11$year, data11$totfatrte, col="hot pink",lty=4)

points(data12$year, data12$totfatrte, col="sea green", pch="+")
lines(data12$year, data12$totfatrte, col="sea green",lty=4)
```
```{r}
hist(data$totfatrte)

hist(data$seatbelt)

hist(data$minage)

hist(data$gdl)

hist(data$zerotol)

hist(data$vehicmiles)

hist(data$unem)

hist(data$perc14_24)

hist(data$vehicmilespc)
```


2. (15%) How is the our dependent variable of interest *totfatrte* defined? What is the average of this variable in each of the years in the time period covered in this dataset? Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004. What does this model explain? Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.

3. (15%) Expand your model in *Exercise 2* by adding variables *bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc*, and perhaps *transformations of some or all of these variables*. Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. How are the variables *bac8* and *bac10* defined? Interpret the coefficients on *bac8* and *bac10*. Do *per se laws* have a negative effect on the fatality rate? What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)

4. (15%) Reestimate the model from *Exercise 3* using a fixed effects (at the state level) model. How do the coefficients on *bac08, bac10, perse, and sbprim* compare with the pooled OLS estimates? Which set of estimates do you think is more reliable? What assumptions are needed in each of these models?  Are these assumptions reasonable in the current context?

5. (10%) Would you perfer to use a random effects model instead of the fixed effects model you built in *Exercise 4*? Please explain.

6. (10%) Suppose that *vehicmilespc*, the number of miles driven per capita, increases by $1,000$. Using the FE estimates, what is the estimated effect on *totfatrte*? Please interpret the estimate.

7. (5%) If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors?













